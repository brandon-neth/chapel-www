<!DOCTYPE html>

<html data-theme="light" lang="en"><head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="#00cbff" name="theme-color"/>
<meta content="A report on developing MCP-based integrations for the Chapel programming language" name="description"/>
<link href="//cdnjs.cloudflare.com/ajax/libs/normalize/5.0.0/normalize.min.css" media="screen,print" rel="stylesheet"/>
<style>.sidenote-checkbox { display: none; }</style>
<style>.feather { width: 1rem; height: 1rem; }</style>
<link href="../../scss/style.min.css" media="screen,print" rel="stylesheet"/>
<link href="../../scss/sidenotes.min.css" media="screen,print" rel="stylesheet"/>
<link href="../../css/syntax.min.css" media="screen,print" rel="stylesheet"/>
<link href="../../scss/syntax-terminal.min.css" media="screen,print" rel="stylesheet"/>
<link href="../../scss/code.min.css" media="screen,print" rel="stylesheet"/>
<link href="../../img/favicon.ico" rel="icon" type="image/png"/>
<script defer="" src="../../js/dropdown-menu.js"></script>
<title>Experimenting with the Model Context Protocol and Chapel</title>
</head>
<body>
<header>
<div class="container">
<a class="site-title" href="../../">
<img alt="Chapel logo" height="50" src="../../img/logo.png" width="50"/>
<h1>Chapel Language Blog</h1>
</a>
</div>
<nav id="Header">
<div class="container">
<a href="../../about">About</a>
<a href="https://chapel-lang.org">Chapel Website</a>
<a href="../../featured">Featured</a>
<a href="../../series">Series</a>
<a href="../../tags">Tags</a>
<a href="../../authors">Authors</a>
<a href="../../posts">All Posts</a>
</div>
</nav>
</header>
<main class="container">
<h2>Experimenting with the Model Context Protocol and Chapel</h2>
<div class="post-subscript">
<p>Posted on August 28, 2025.</p>
<p>
        Tags:
        
        <a class="button" href="../../tags/ai/ml">AI/ML</a>
<a class="button" href="../../tags/tools">Tools</a>
<a class="button" href="../../tags/how-to">How-To</a>
</p>
<p>
    By:
    <a href="../../authors/daniel-fedorin">Daniel Fedorin</a>
</p>
</div>
<div class="post-content">
<div class="table-of-contents">
<div class="wrapper">
<span class="header">Table of Contents</span>
<nav id="TableOfContents">
<ul>
<li><a href="#mcp-and-large-language-models">MCP and Large Language Models</a></li>
<li><a href="#tools-in-chapels-mcp-prototype">Tools in Chapel’s MCP Prototype</a></li>
<li><a href="#mcp-in-the-age-of-agents">MCP in the Age of Agents</a></li>
<li><a href="#conclusions-and-looking-ahead">Conclusions and Looking Ahead</a></li>
</ul>
</nav>
</div>
</div>
<p>Developer tooling built on Large Language Models (LLMs) is a popular, if
controversial, topic nowadays. Generative models have proven themselves capable
of generating code in a variety of languages, and they can help newcomers
and experts alike. We have been intrigued to explore the possibilities of using
LLMs for writing Chapel code. For a novel language like Chapel, however, there are some
challenges when it comes to working with LLM-based tools.</p>
<ul>
<li>Chapel aims to advance the state of the art when it comes to parallel
computing. This means — almost by definition — that far less Chapel code is
available, compared to “conventional” languages like Python and C++, in
training datasets that power LLMs.</li>
<li>Prior to its <a href="../../posts/announcing-chapel-2.0/">2.0 release</a>, Chapel
evolved rapidly and in backwards-incompatible ways, meaning that those sample
programs that were
<span class="sidenote"><label class="sidenote-label" for="sidenote-0">used for LLM training</label><input class="sidenote-checkbox" id="sidenote-0" type="checkbox"/><span class="sidenote-content sidenote-right"><span class="sidenote-delimiter">[note:</span>ChatGPT at the time of writing (August 27, 2025) reports its knowledge cutoff
to be June 2024. The 2.0 release of Chapel occurred in March of 2024. As a
result, there are only a few months worth of post-2.0 code available for
training.<span class="sidenote-delimiter">]</span></span></span> may no longer be a good
representation of proper code practices today.</li>
</ul>
<p>Fortunately, LLMs and their surrounding tools are continuously becoming more capable,
and we had some luck supplementing them with up-to-date and accurate information
about Chapel. To do so, we leaned on the The Model Context
Protocol (MCP), which is a standardized way for LLM-based tooling to go beyond token
prediction, and interact in various ways with “the outside world.” The
protocol is supported by Visual Studio Code, Anthropic’s Claude and Claude Code, Zed,
and a variety of other software. This post details our recent experiments with
using MCP to help address the challenges I’ve outlined above.</p>
<p>This post can be read in two ways. In one sense, it describes what users
writing Chapel might be able to do to improve their LLM-enabled workflow.
At the same time, the challenges above would be shared by any other smaller,
novel language. Thus, in another sense, this post is an LLM experience report
by us as language developers for anyone else who is seeking to push the landscape
of programming languages forward, as we are.</p>
<blockquote class="pull-quote">
<div class="quote-wrapper">
<div class="quote-container"><span class="open-quote">“</span></div>
<div class="quote-content">
<p>
Even if the training of Claude were to permanently stop right now, an
MCP-based tool could provide it accurate information in perpetuity.
</p>
</div>
<div class="quote-container"><span class="close-quote">”</span></div>
</div>
</blockquote>
<h3 id="mcp-and-large-language-models">
<a href="#mcp-and-large-language-models">MCP and Large Language Models</a>
</h3>
<p>The <a href="https://modelcontextprotocol.io" rel="noopener" target="_blank">Model Context Protocol</a> provides a way to populate an LLM-based assistant’s
toolbox with actions it can take. Each action is typically called a <em>tool</em>.
For instance, in the image below, I’ve configured <a href="https://claude.ai/" rel="noopener" target="_blank">Claude</a>
to use the experimental <a href="https://github.com/DanilaFe/chapel-support" rel="noopener" target="_blank">Chapel MCP server</a>.
As a result, it has access to five Chapel-specific tools that I’ll describe
in this article.</p>
<figure class="fullwide"><img alt="The Chapel section of Claude’s tool menu" src="../../posts/claude-mcp/mcp-menu.png"/><figcaption>
<p>The Chapel section of Claude’s tool menu</p>
</figcaption>
</figure>
<p>When it’s constructing its answer, the assistant can choose to invoke the tools
that have been enabled for it. For instance, I could ask Claude to tell me
the first “real” line of code in <a href="https://chapel-lang.org/docs/2.4/primers/fileIO.html" rel="noopener" target="_blank">the <code>fileIO</code> primer</a>.
Instead of searching or guessing, Claude is able to access the primer file.</p>
<figure class="fullwide"><img alt="Claude accessing the fileIO primer" src="../../posts/claude-mcp/using-primers.png"/><figcaption>
<p>Claude accessing the <code>fileIO</code> primer</p>
</figcaption>
</figure>
<p>Eventually, it arrives at the answer:</p>
<figure class="fullwide"><img alt="Claude reporting the first line in the fileIO primer" src="../../posts/claude-mcp/using-primers-answer.png"/><figcaption>
<p>Claude reporting the first line in the <code>fileIO</code> primer</p>
</figcaption>
</figure>
<p>This is indeed the case.</p>
<p>Actions performed in this way are more resilient against hallucinations or
<span class="sidenote"><label class="sidenote-label" for="sidenote-1">knowledge cut-offs,</label><input class="sidenote-checkbox" id="sidenote-1" type="checkbox"/><span class="sidenote-content sidenote-right" style="margin-top: -7.5rem"><span class="sidenote-delimiter">[note:</span>A <em>knowledge cut-off</em> is the point in time after which no information has
been used to train the language model. In practice, this means the model
hasn’t seen anything that has occurred or been created after that point.<span class="sidenote-delimiter">]</span></span></span>
since they provide grounded information that is not simply encoded in the model’s
weights. Even if the training of Claude (e.g.) were to permanently stop
right now, an MCP-based tool could provide it accurate information in perpetuity.</p>
<p>When working with a smaller language like Chapel, the ability to access
documentation in the form of primers (via the MCP’s <code>get_primer</code> action), saves
an assistant like Claude from having to “guess” correct syntax or features, since
it can consult a vetted example instead. To also save it from guessing
what vetted examples are available, we also give it <code>list_primers</code>.
Here’s
<span class="sidenote"><label class="sidenote-label" for="sidenote-2">another example</label><input class="sidenote-checkbox" id="sidenote-2" type="checkbox"/><span class="sidenote-content sidenote-right" style="margin-top: -6.5rem"><span class="sidenote-delimiter">[note:</span>This example might seem contrived, and it is. My goal was to ask the model
to write a program that hasn’t been written before, to reduce the chances
of it simply regurgitating an example that was found somewhere in its
training set.<br/><br/>Also, I’ve included my full prompt in the interest of transparency. You
can see that I’ve asked the model not to pull in more than one primer. This
is mostly because I’m using a free Claude account to experiment, and thus
am subject to limitations on chat length.<span class="sidenote-delimiter">]</span></span></span>
of Claude using Chapel’s MCP tools.</p>
<figure class="fullwide"><img alt="Claude accessing the fileIO primer to help it write code" src="../../posts/claude-mcp/using-primers-again.png"/><figcaption>
<p>Claude accessing the <code>fileIO</code> primer to help it write code</p>
</figcaption>
</figure>
<p>Above, the assistant used another Chapel-provided tool, <code>compile_program</code>,
to test if the code it wrote was valid. Since the standalone Claude assistant
is confined to the chat window, it could not run the program to check for
runtime errors. A coding <em>agent</em> would not be subject to such limitations;
we’ll talk about those in a little bit.</p>
<p>In a more exciting example, I asked Claude to generate a “Conway’s Game of Life”
program. Mostly, I just wanted something that lent itself to pretty visuals.
I did so as part of <a href="https://www.youtube.com/watch?v=zOxD4VmSE5o" rel="noopener" target="_blank">a public demonstration</a>
that covers much of the same content that
we’ve discussed here. There too, Claude used the primers (it requested the <code>forallLoops</code>
primer), compiled the program, found an error, fixed it, recompiled, and
even linted the resulting code. The result was notable for two reasons:</p>
<ul>
<li>
<p>Although Chapel has a test case implementing the Game of Life, it was clear that Claude
did not simply regurgitate it while writing the code — there were numerous
significant differences.</p>
</li>
<li>
<p>In some of my runs — depending on my prompt — Claude generated an animated
visualization of the cells. The prettiest one (they were all different)
included borders generated with Unicode characters. Here’s what that looked like:</p>
<figure><img alt="A terminal-based animation of Conway’s game of life generated by a Chapel program Claude wrote" src="../../posts/claude-mcp/conwaygif.gif"/><figcaption>
<p>A terminal-based animation of Conway’s game of life generated by a Chapel program Claude wrote</p>
</figcaption>
</figure>
</li>
</ul>
<h3 id="tools-in-chapels-mcp-prototype">
<a href="#tools-in-chapels-mcp-prototype">Tools in Chapel’s MCP Prototype</a>
</h3>
<p>In my screenshots and links I’ve been showcasing my MCP server prototype that
I wrote while playing with Chapel and LLMs. It is open source, so you
can install it and try it out yourself.</p>
<details>
<summary><strong>(How do I set up the MCP prototype on my machine?)</strong></summary>
<div style="padding-left: 2vw; padding-right: 2vw; padding-bottom: 2ch;">
<p>The repository for Chapel’s prototype MCP server is <a href="https://github.com/DanilaFe/chapel-support" rel="noopener" target="_blank">here at the time of writing</a>. Please refer  to its <code>README.md</code> file for installation instructions. In
short, you can use <code>uv</code> to set up the project and install the necessary
dependencies.</p>
<p>From there, you must configure the MCP server within your LLM-enabled tools.
I’ve set it up in Visual Studio Code, Claude, and Claude Code. In pretty
much all of these, you need to write some sort of JSON configuration.</p>
<ul>
<li>
<p><strong>VSCode</strong>: I had to look up <code>mcp</code> in the Settings search box, where I
was able to find a link to <code>settings.json</code>. In there, I added the following:</p>
<pre tabindex="0"><code>"mcp": {
    "inputs": [],
    "servers": {
        "chapel-support": {
            "command": "uv",
            "args": [
                "--directory",
                "/path/to/chapel/mcp/server",
                "run",
                "chapel-support.py"
            ],
            "env": {}
        }
    }
}
</code></pre></li>
<li>
<p><strong>Claude</strong>: One must use the desktop Claude app, rather than the web version,
since only the desktop application supports MCP. I had to edit
<code>claude_desktop_config.json</code> to add:</p>
<pre tabindex="0"><code>"mcpServers": {
    "chapel-support": {
        "command": "uv",
        "args": [
            "--directory",
            "/path/to/chapel/mcp/server",
            "run",
            "chapel-support.py"
        ]
    }
}
</code></pre></li>
<li>
<p><strong>Claude Code</strong>: I had to modify <code>~/.claude.json</code>. The relevant piece is:</p>
<pre tabindex="0"><code>"mcpServers": {
    "chapel-support": {
        "type": "stdio",
        "command": "uv",
        "args": [
            "--directory",
            "/path/to/chapel/mcp/server",
            "run",
            "chapel-support.py"
        ],
        "env": {}
    }
}
</code></pre></li>
</ul>
<p>In Zed, there is currently <a href="https://github.com/zed-industries/zed/pull/30178#issuecomment-2861794830" rel="noopener" target="_blank">a bug</a>
with MCP servers and GitHub Copilot Chat, which is the precise combination
that I was using. As a result, I don’t have a way to validate that my config was
working.</p>
</div>
</details>
<p>I’d now like to give a more
detailed description of the tools I’ve provided at the time of writing, and
give some rationale for including them.</p>
<ul>
<li>
<p><code>list_primers</code> and <code>get_primers</code> serve to provide the model with up-to-date
examples of Chapel code. This approach is very valuable for languages that are
relatively underrepresented in a model’s training dataset. It also
helps decouple the language’s evolution from model updates on the side
of the LLM provider: if Chapel’s best practices were to change the day
after a model update, an updated MCP server can immediately supplement
the model’s now-stale knowledge.</p>
</li>
<li>
<p><code>compile_program</code> is used to invoke the Chapel compiler on a piece of code
created by the LLM. For users who interact with LLMs via a chat-based
assistant (transferring code and changes to their editor), this can
save on the number of round-trips in case the model makes a mistake.</p>
<p>Even without my prompting (see screenshot above), Claude chose to compile
the program to verify that it works. If it had written incorrect code
(as it had in other experiments of mine), it would keep trying, instead
of simply concluding its response.</p>
<figure class="fullwide"><img alt="Claude finding errors in its code by compiling a program" src="../../posts/claude-mcp/compile-program.png"/><figcaption>
<p>Claude finding errors in its code by compiling a program</p>
</figcaption>
</figure>
<p>As I will discuss below, AI Agents (such as Claude code) can do this without
specialized MCP servers. In that way, <code>compile_command</code> has a more narrow
niche. However, for
<span class="sidenote"><label class="sidenote-label" for="sidenote-3">users that rely on chat-based interaction unenhanced by agents,</label><input class="sidenote-checkbox" id="sidenote-3" type="checkbox"/><span class="sidenote-content sidenote-right" style="margin-top: -6.5rem"><span class="sidenote-delimiter">[note:</span>Anecdotally, many of the people I know still do. <a href="https://fly.io/blog/youre-all-nuts/" rel="noopener" target="_blank">Apparently</a>, this
means they aren’t doing it like you’re supposed to; pragmatically, though,
that means there’s room for improving these folks’ experience with MCP supplements.<span class="sidenote-delimiter">]</span></span></span>
the user experience improvement is worthwhile.</p>
</li>
<li>
<p><code>list_chapel_lint_rules</code> and <code>lint_chapel_code</code> help catch stylistic issues
in code produced by the LLM, almost like an early stage in code review.
If not all of the Chapel code in an LLM’s data set is valid in modern Chapel,
even less of all such code fits the recommended stylistic conventions.</p>
<p>These tools leverage <a href="https://chapel-lang.org/docs/tools/chplcheck/chplcheck.html" rel="noopener" target="_blank"><code>chplcheck</code></a>
to serve a similar function to <code>compile_program</code> above.</p>
<figure class="fullwide"><img alt="Claude finding linter warnings in its code" src="../../posts/claude-mcp/lint-program.png"/><figcaption>
<p>Claude finding linter warnings in its code</p>
</figcaption>
</figure>
<p>As with <code>compile_program</code>, this reduced the effort required from a user
to update, clean up, or integrate generated code. Since the LLM will tend to keep
working on its code as long as it has linter warnings to address, the
final product will be in better shape for human review.</p>
</li>
</ul>
<h3 id="mcp-in-the-age-of-agents">
<a href="#mcp-in-the-age-of-agents">MCP in the Age of Agents</a>
</h3>
<p>If I had a dollar for every time I saw “Agentic” on my LinkedIn feed, I
could probably retire right now. AI Agents are an application of LLMs that
independently iterate on tasks, interact with their environment, and
autonomously work towards some goal. <a href="https://www.anthropic.com/claude-code" rel="noopener" target="_blank">Claude Code</a>,
which I’ve mentioned a few times in this post, and which is not the same
as regular Claude, is one agent I’ve played with. It sits in your terminal
and interacts with it like a human does: it types commands, browses the
file system, writes some code, compiles it, etc. For such an agent, there is
no need to perform an MCP call to <code>compile_program</code>, because it can just
run <code>chpl</code> (the compiler) from the terminal. Better yet, since it’s working
in your project directory, such an agent could figure out that you use
a <code>Makefile</code>, or <a href="https://chapel-lang.org/docs/tools/mason/mason.html" rel="noopener" target="_blank">Mason</a>,
and
<span class="sidenote"><label class="sidenote-label" for="sidenote-4">run the corresponding build commands.</label><input class="sidenote-checkbox" id="sidenote-4" type="checkbox"/><span class="sidenote-content sidenote-right"><span class="sidenote-delimiter">[note:</span>In fact, when building <a href="https://github.com/bears-r-us/arkouda" rel="noopener" target="_blank">Arkouda</a> as
part of one of my experiments, my agent observed that a “dependency check”
step in the build process that was running each time was taking too long. So,
it found an environment variable that disables this check (which I had no idea about),
and started using it.<span class="sidenote-delimiter">]</span></span></span>
It could also invoke the <code>chplcheck</code>
linter, read its output, and fix the necessary warnings. This is what I meant
earlier by agents “not needing” <code>compile_program</code> and the like.</p>
<p>For such agents, I still believe the <code>list_primers</code> and <code>get_primer</code> commands,
along with any future documentation-retrieval features, are useful. General
documentation such as the primers almost certainly sits outside of
any given project’s structure, so a naive search of the working directory
might not find it. In a similar vein to what I described above, another
advantage to standalone documentation is that updates to it can be decoupled
from updates to the project, ensuring its freshness.</p>
<p>One might argue that providing documentation like the primers via a special
MCP server is just a special case of <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation" rel="noopener" target="_blank">Retrieval-Augmented Generation</a> (RAG) or internet search. The primers are available via search engines,
after all, so why bother developing an MCP server?</p>
<p>I concede that in the future, agents may simply search
the web as humans do for relevant documentation. It’s plausible to me that
they will distinguish “official” documentation from unofficial resources,
find the appropriate pages, and so on. In a move reminiscent of
<a href="https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf" rel="noopener" target="_blank">the bitter lesson</a>,
general-purpose search might supplant domain-specific knowledge databases like
MCP-provided primers. However, at the time of writing, I see distinct
advantages to the current, MCP-based approach:</p>
<ul>
<li><strong>Lower latency</strong>: I’ve watched AI assistants perform internet searches, and
in my experience, they are quite slow. It takes time to poll the web,
prioritize sources, and synthesize a response. A local MCP server can
short-circuit the process of discovering and accessing documentation
in a way that doesn’t even require an internet connection.</li>
<li><strong>Focus</strong>: If you’re performing a search on the internet, the results
for your query are provided and ranked by the search engine’s heuristics.
This makes it possible to manipulate what data a model will get back. If
you’re using local search, perhaps via <a href="https://www.ibm.com/think/topics/vector-embedding" rel="noopener" target="_blank">vector embeddings</a>,
the argument is symmetric — only the heuristic changes.</li>
<li><strong>Uniformity</strong>: Search-based systems today are proprietary tools
that augment language models. This means that their behavior can vary according
to the provider (OpenAI, Anthropic, GitHub, etc.). A single MCP server, on
the other hand, will provide the same results to all models, and thus work
more consistently.</li>
</ul>
<p>Given the rate at which LLM tooling is evolving today, it’s hard to predict what the landscape
will look like even a year from now. However, for the time being, agentic or
not, an MCP server for Chapel seems to be a useful addition to an LLM’s repertoire.</p>
<h3 id="conclusions-and-looking-ahead">
<a href="#conclusions-and-looking-ahead">Conclusions and Looking Ahead</a>
</h3>
<p>The MCP server for Chapel is the result of a relatively short-term investigation.
As a result, it’s in its early stages, and we are still looking to evaluate
its effectiveness. However, it has shown promising results for both chat-based
and agentic LLM tools.</p>
<p>There are still areas within MCP for Chapel’s server to explore. As I demonstrated
above, in all of my experiments, I had given instructions to the LLM to use
tools like <code>get_primer</code> (though at times, the model chose to do so on its own).
MCP allows servers to provide pre-written prompts to their users. Rather than
having to read my post and know to say “use the Chapel tools you have”, a
user might be able to leverage pre-written prompts that include such instructions.</p>
<p>Another interesting area within MCP is to provide the model with the same
IntelliSense that editors enable for their users. Both Visual Studio Code
and Zed have support for feeding diagnostics into the LLM:</p>
<figure class="fullwide"><img alt="Claude accessing errors and warnings in a file using Zed’s diagnostic tool" src="../../posts/claude-mcp/zed-diagnostics.png"/><figcaption>
<p>Claude accessing errors and warnings in a file using Zed’s diagnostic tool</p>
</figcaption>
</figure>
<p>This support is not universal — it depends on the editor, LLM provider, etc.
In the future, it would be interesting to explore using Chapel’s MCP server
to provide this information to MCP-compatible clients, using <a href="https://chapel-lang.org/docs/tools/chpl-language-server/chpl-language-server.html" rel="noopener" target="_blank"><code>chpl-language-server</code></a>. Other potentially-useful aspects
of Chapel programs, such as go-to-definition and on-hover documentation, could
also be provided by the server to the model.</p>
<p>Please <a href="https://chapel-lang.org/community/" rel="noopener" target="_blank">reach out to us</a> if you have
feedback or requests for the server. We’d love to hear from you!</p>
</div>
</main>
<div class="container">
<div class="share-view">
<h3>Share this article:</h3>
<div class="share-buttons">
<a class="button share-button" href="https://bsky.app/intent/compose?text=Check+out+this+post+entitled+%22Experimenting+with+the+Model+Context+Protocol+and+Chapel%22+on+the+Chapel+Programming+Language+blog%3A+https%3A%2F%2Fchapel-lang.org%2Fblog%2Fposts%2Fclaude-mcp%2F" rel="noopener noreferrer" style="--button-color: #6cb0f9; --button-color-light: white;" target="_blank">
<img alt="Share on BlueSky" height="30" src="../../img/bluesky-logo.jpg" width="30"/>
</a>
<a class="button share-button" href="https://www.facebook.com/sharer/sharer.php?description=Check+out+this+post+entitled+%22Experimenting+with+the+Model+Context+Protocol+and+Chapel%22+on+the+Chapel+Programming+Language+blog%3A&amp;u=https%3A%2F%2Fchapel-lang.org%2Fblog%2Fposts%2Fclaude-mcp%2F" rel="noopener noreferrer" style="--button-color: #3a559f; --button-color-light: white;" target="_blank">
<img alt="Share on Facebook" height="30" src="../../img/facebook-logo.png" width="30"/>
</a>
<a class="button share-button" href="https://linkedin.com/share?text=Check+out+this+post+entitled+%22Experimenting+with+the+Model+Context+Protocol+and+Chapel%22+on+the+Chapel+Programming+Language+blog%3A&amp;url=https%3A%2F%2Fchapel-lang.org%2Fblog%2Fposts%2Fclaude-mcp%2F" rel="noopener noreferrer" style="--button-color: #2867b2; --button-color-light: white;" target="_blank">
<img alt="Share on LinkedIn" height="30" src="../../img/linkedin-logo.png" width="30"/>
</a>
<a class="button share-button" href="https://new.reddit.com/submit?title=Experimenting+with+the+Model+Context+Protocol+and+Chapel&amp;url=https%3A%2F%2Fchapel-lang.org%2Fblog%2Fposts%2Fclaude-mcp%2F" rel="noopener noreferrer" style="--button-color: #ff4500; --button-color-light: white;" target="_blank">
<img alt="Share on Reddit" height="30" src="../../img/reddit-logo.svg" width="30"/>
</a>
<a class="button share-button" href="http://x.com/share?text=Check+out+this+post+entitled+%22Experimenting+with+the+Model+Context+Protocol+and+Chapel%22+on+the+Chapel+Programming+Language+blog%3A&amp;url=https%3A%2F%2Fchapel-lang.org%2Fblog%2Fposts%2Fclaude-mcp%2F" rel="noopener noreferrer" style="--button-color: #000000; --button-color-light: #7a7a7a;" target="_blank">
<img alt="Share on X" height="30" src="../../img/x-logo.svg" width="30"/>
</a>
</div>
</div>
</div>
</body>
</html>
